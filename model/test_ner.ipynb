{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "try:\n",
    "    from vlsp import read_and_process\n",
    "    from utils.ner_utils import tokenize_and_align \n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../tokenizer/trained_tokenizer/tokenizer-50k\")\n",
    "\n",
    "data_type = \"test\"\n",
    "\n",
    "text_files = sorted(glob(os.path.join(\"../vlsp/data/formatted_data/2016\", data_type, \"**\", \"*.txt\"), recursive=True))\n",
    "df = read_and_process.read_format_and_return_df(text_files, remove_tags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 2831/2831 [00:02<00:00, 1310.92 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['words', 'tags', 'input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
      "    num_rows: 2831\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_spark(df, split=\"train\")\n",
    "ds = ds.with_format(\"torch\")\n",
    "\n",
    "label_map = read_and_process.get_label_map()\n",
    "\n",
    "ds = ds.map(tokenize_and_align, batched=True, num_proc=16, fn_kwargs={\"tokenizer\": tokenizer, \"label_map\": label_map})\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([\"words\", \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
       "    num_rows: 2831\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2831/2831 [00:00<00:00, 270695.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds.save_to_disk(f\"../data_all/data_extra/data_ner/{data_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
