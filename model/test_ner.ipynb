{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/28 11:37:54 WARN Utils: Your hostname, hyle-nitro resolves to a loopback address: 127.0.1.1; using 192.168.0.216 instead (on interface wlp0s20f3)\n",
      "24/11/28 11:37:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/28 11:37:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "try:\n",
    "    from vlsp import read_and_process\n",
    "    from utils.ner_utils import tokenize_and_align \n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../tokenizer/trained_tokenizer/tokenizer-50k\")\n",
    "\n",
    "\n",
    "text_files = sorted(glob(os.path.join(\"../vlsp/data/formatted_data/2016\", \"**\", \"*.txt\"), recursive=True))\n",
    "df = read_and_process.read_format_and_return_df(text_files, remove_tags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 19687/19687 [00:05<00:00, 3827.71 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['words', 'tags', 'input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
      "    num_rows: 19687\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_spark(df, split=\"train\")\n",
    "ds = ds.with_format(\"torch\")\n",
    "\n",
    "label_map = read_and_process.get_label_map()\n",
    "\n",
    "ds = ds.map(tokenize_and_align, batched=True, num_proc=16, fn_kwargs={\"tokenizer\": tokenizer, \"label_map\": label_map})\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_id \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m attn \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(input_id))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "input_id = ds[\"input_ids\"][0]\n",
    "attn = ds[\"attention_mask\"][0]\n",
    "print(type(input_id))\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "tag = ds[\"tags\"][0]\n",
    "\n",
    "print(tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
