{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    NormalizedString,\n",
    "    Regex,\n",
    "    AddedToken, \n",
    ")\n",
    "from tokenizers.normalizers import Normalizer\n",
    "from transformers import AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unicode normalization \n",
    "2. Mapping from 'y' to 'i (bác sỹ -> bác sĩ) except when 'y' is alone or when it's in the compound 'uy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thy']\n"
     ]
    }
   ],
   "source": [
    "test = \"thy\"\n",
    "test_pattern = re.compile(r\"(?:[th])*y\")\n",
    "\n",
    "print(re.findall(test_pattern, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ỹ', 'ỹ', 'ỹ', 'ý', 'y']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"(?<=\\b[hklmnst])(?:\\S*)(?<=[^a\\á\\à\\ạ\\ả\\ã\\â\\ấ\\ầ\\ậ\\ẩ\\ẫ\\ă\\ắ\\ằ\\ẳ\\ẵ\\ặu\\ú\\ù\\ụ\\ủ\\ũ\\s])([y\\ỷ\\ỹ\\ỵ\\ỳ\\ý])\\b\")\n",
    "test_text = \"bác sỹ, thạc sỹ và ca sỹ, thý thoy mình bị suy dinh dưỡng.\"\n",
    "\n",
    "\n",
    "print(re.findall(pattern, test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings import normalize_encoding\n",
    "\n",
    "\n",
    "class CustomNormalizer: \n",
    "    def __init__(self): \n",
    "        self.pattern = re.compile(r\"(?<=\\b[hklmnst])(?:\\S*)(?<=[^a\\á\\à\\ạ\\ả\\ã\\â\\ấ\\ầ\\ậ\\ẩ\\ẫ\\ă\\ắ\\ằ\\ẳ\\ẵ\\ặu\\ú\\ù\\ụ\\ủ\\ũ\\s])([y\\ỷ\\ỹ\\ỵ\\ỳ\\ý])\\b\")\n",
    "        self.y_to_i_map = {\n",
    "            'y': 'i',\n",
    "            'ỷ': 'ỉ',\n",
    "            'ỹ': 'ĩ',\n",
    "            'ỵ': 'ị',\n",
    "            'ỳ': 'ì',\n",
    "            'ý': 'í'\n",
    "        }\n",
    "\n",
    "    def normalize(self, normalized: NormalizedString): \n",
    "        original = normalized.original\n",
    "        matches = self.pattern.findall(original)\n",
    "\n",
    "        for match in matches: \n",
    "            normalized.replace(match, self.y_to_i_map[match])\n",
    "\n",
    "        print(normalized.normalized)\n",
    "        normalized.nfc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bác sỹ, thạc sỹ và ca sỹ, thý thoy mình bị suy dinh dưỡng.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"bác sỹ, thạc sỹ và ca sỹ, thý thoy mình bị suy dinh dưỡng.\"\n",
    "norm = NormalizedString(test_text)\n",
    "norm.lowercase()\n",
    "\n",
    "norm.normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ý']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(re.compile(r\"(?<=\\b[hklmnst])(?:\\S*)(?<=[^a\\á\\à\\ạ\\ả\\ã\\â\\ấ\\ầ\\ậ\\ẩ\\ẫ\\ă\\ắ\\ằ\\ẳ\\ẵ\\ặu\\ú\\ù\\ụ\\ủ\\ũ\\s])(ý)\\b\"), test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE(unk_token=None, fuse_unk=False, dropout=None, end_of_word_suffix=\"\", continuing_subword_prefix=\"\", byte_fallback=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_sequence = normalizers.Sequence([\n",
    "    normalizers.NFC(), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenizer_sequence = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Split(Regex(\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"), behavior=\"isolated\", invert=False),\n",
    "    pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = AddedToken(content=\"<CLS>\", lstrip=False, normalized=False, rstrip=False, single_word=True)\n",
    "pad = AddedToken(content=\"<PAD>\", lstrip=False, normalized=False, rstrip=False, single_word=True)\n",
    "mask = AddedToken(content=\"<MASK>\", lstrip=False, normalized=False, rstrip=False, single_word=True)\n",
    "sep = AddedToken(content=\"<SEP>\", lstrip=False, normalized=False, rstrip=False, single_word=True)\n",
    "unk = AddedToken(content=\"<UNK>\", lstrip=False, normalized=False, rstrip=False, single_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"arrow\", data_files=\"../data_all/data/data_00002.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_dataset[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer_sequence\n",
    "tokenizer.pre_tokenizer = pretokenizer_sequence\n",
    "tokenizer.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (30000 + 261 + 64 - 53) \n",
    "vocab_size = 30272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=500, show_progress=True, max_token_length=2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(training_corpus, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config tokenizer post processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add special tokens \n",
    "\n",
    "tokenizer.add_special_tokens([cls, pad, mask, sep, unk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable padding \n",
    "tokenizer.enable_padding(pad_id=501, pad_type_id=501, pad_token=\"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable truncation \n",
    "tokenizer.enable_truncation(max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set postprocessors\n",
    "postprocessor_sequence = processors.Sequence([\n",
    "    processors.ByteLevel(trim_offsets=False), \n",
    "    processors.TemplateProcessing(\n",
    "    single=\"<CLS> $0 <SEP>\",\n",
    "    pair=\"<CLS> $A <SEP> $B:1 <SEP>:1\",\n",
    "    special_tokens=[(\"<CLS>\", 15000), (\"<SEP>\", 15003)]),\n",
    "])\n",
    "\n",
    "tokenizer.post_processor = postprocessor_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=TruncationParams(direction=Right, max_length=2048, strategy=LongestFirst, stride=0), padding=PaddingParams(strategy=BatchLongest, direction=Right, pad_to_multiple_of=None, pad_id=501, pad_type_id=501, pad_token=\"<PAD>\"), added_tokens=[{\"id\":15000, \"content\":\"<CLS>\", \"single_word\":True, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":15001, \"content\":\"<PAD>\", \"single_word\":True, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":15002, \"content\":\"<MASK>\", \"single_word\":True, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":15003, \"content\":\"<SEP>\", \"single_word\":True, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":15004, \"content\":\"<UNK>\", \"single_word\":True, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=Sequence(normalizers=[NFC()]), pre_tokenizer=Sequence(pretokenizers=[Split(pattern=Regex(\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"), behavior=Isolated, invert=False), ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=False)]), post_processor=Sequence(processors=[ByteLevel(add_prefix_space=True, trim_offsets=False, use_regex=True), TemplateProcessing(single=[SpecialToken(id=\"<CLS>\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"<SEP>\", type_id=0)], pair=[SpecialToken(id=\"<CLS>\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"<SEP>\", type_id=0), Sequence(id=B, type_id=1), SpecialToken(id=\"<SEP>\", type_id=1)], special_tokens={\"<CLS>\":SpecialToken(id=\"<CLS>\", ids=[15000], tokens=[\"<CLS>\"]), \"<SEP>\":SpecialToken(id=\"<SEP>\", ids=[15003], tokens=[\"<SEP>\"])})]), decoder=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=True, ignore_merges=False, vocab={\"!\":0, \"\"\":1, \"#\":2, \"$\":3, \"%\":4, \"&\":5, \"'\":6, \"(\":7, \")\":8, \"*\":9, \"+\":10, \",\":11, \"-\":12, \".\":13, \"/\":14, \"0\":15, \"1\":16, \"2\":17, \"3\":18, \"4\":19, \"5\":20, \"6\":21, \"7\":22, \"8\":23, \"9\":24, \":\":25, \";\":26, \"<\":27, \"=\":28, \">\":29, \"?\":30, \"@\":31, \"A\":32, \"B\":33, \"C\":34, \"D\":35, \"E\":36, \"F\":37, \"G\":38, \"H\":39, \"I\":40, \"J\":41, \"K\":42, \"L\":43, \"M\":44, \"N\":45, \"O\":46, \"P\":47, \"Q\":48, \"R\":49, \"S\":50, \"T\":51, \"U\":52, \"V\":53, \"W\":54, \"X\":55, \"Y\":56, \"Z\":57, \"[\":58, \"\\\":59, \"]\":60, \"^\":61, \"_\":62, \"`\":63, \"a\":64, \"b\":65, \"c\":66, \"d\":67, \"e\":68, \"f\":69, \"g\":70, \"h\":71, \"i\":72, \"j\":73, \"k\":74, \"l\":75, \"m\":76, \"n\":77, \"o\":78, \"p\":79, \"q\":80, \"r\":81, \"s\":82, \"t\":83, \"u\":84, \"v\":85, \"w\":86, \"x\":87, \"y\":88, \"z\":89, \"{\":90, \"|\":91, \"}\":92, \"~\":93, \"¡\":94, \"¢\":95, \"£\":96, \"¤\":97, \"¥\":98, ...}, merges=[(\"á\", \"»\"), (\"á\", \"º\"), (\"n\", \"g\"), (\"Ġ\", \"t\"), (\"Ġ\", \"c\"), (\"Ġ\", \"Ä\"), (\"n\", \"h\"), (\"ĠÄ\", \"ĳ\"), (\"Ã\", \"ł\"), (\"Æ\", \"°\"), (\"Ġt\", \"h\"), (\"Ã\", \"¡\"), (\"Ġ\", \"v\"), (\"Ġ\", \"l\"), (\"Ġ\", \"h\"), (\"Ġc\", \"h\"), (\"i\", \"á»\"), (\"Ġ\", \"m\"), (\"Ġ\", \"b\"), (\"Ġ\", \"k\"), (\"Ġt\", \"r\"), (\"Æ°\", \"á»\"), (\"Ġ\", \"nh\"), (\"Ġ\", \"s\"), (\"áº\", \"¡\"), (\"áº\", \"¿\"), (\"Ã\", \"´\"), (\"Ġ\", \"n\"), (\"Ġ\", \"g\"), (\"Ġk\", \"h\"), (\"áº\", \"£\"), (\"Ġ\", \"d\"), (\"Ã\", \"ª\"), (\"Ã\", \"³\"), (\"á»\", \"Ļ\"), (\"Ġ\", \"ng\"), (\"áº\", \"¥\"), (\"Ġ\", \"p\"), (\"Ġp\", \"h\"), (\"á»\", \"ĳ\"), (\"iá»\", \"ĩ\"), (\"Ã\", \"¢\"), (\"Ã¡\", \"c\"), (\"Ġ\", \"T\"), (\"Ãª\", \"n\"), (\"Ã´\", \"ng\"), (\"Ã\", \"¬\"), (\"ĠÄĳ\", \"á»\"), (\"Ġv\", \"á»\"), (\"Ġv\", \"Ãł\"), (\"Æ\", \"¡\"), (\"á»\", \"§\"), (\"áº\", \"Ń\"), (\"áº\", \"§\"), (\"Ġg\", \"i\"), (\"Ġl\", \"Ãł\"), (\"Ã\", \"Ń\"), (\"u\", \"y\"), (\"Ġth\", \"á»\"), (\"i\", \"áº¿\"), (\"Ġ\", \"q\"), (\"Ġ\", \"r\"), (\"Ľ\", \"i\"), (\"Æ°á»\", \"Ŀ\"), (\"Æ°á»\", \"£\"), (\"áº¥\", \"t\"), (\".\", \"Ċ\"), (\"Ġt\", \"á»\"), (\"Ġ\", \"x\"), (\"Ġ\", \"C\"), (\"Ġ\", \"N\"), (\"Ġc\", \"Ã³\"), (\"Ġc\", \"á»§\"), (\"áº¡\", \"i\"), (\"Ġcá»§\", \"a\"), (\"Ġq\", \"u\"), (\"Ä\", \"ĥ\"), (\"Ã\", \"º\"), (\"Ġh\", \"á»\"), (\"o\", \"ng\"), (\"Ġc\", \"Ã¡c\"), (\"a\", \"n\"), (\"áº\", \"¯\"), (\"á»\", \"ĭ\"), (\"Ã\", \"£\"), (\"Ã¬\", \"nh\"), (\"iá»\", \"ģ\"), (\"á»\", \"¯\"), (\"Æ°á»£\", \"c\"), (\"á»\", \"±\"), (\"Ġ\", \"H\"), (\"Ã¢\", \"n\"), (\"c\", \"h\"), (\"á»Ļ\", \"t\"), (\"á»\", \"¥\"), (\"ĠÄĳ\", \"Æ°á»£c\"), (\"á»\", \"©\"), (\"Ã\", \"²\"), (\"Ġkh\", \"Ã´ng\"), ...]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15003"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"<SEP>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode_batch([raw_dataset[\"train\"][0][\"text\"], raw_dataset[\"train\"][1][\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<CLS>NamĠsinhĠbá»ĭĠbáº¡nĠÄĳÃ¢mĠtrá»įngĠthÆ°Æ¡ngĊNamĠsinhĠbá»ĭĠbáº¡nĠÄĳÃ¢mĠtrá»įngĠthÆ°Æ¡ngĠtrongĠgiá»ĿĠraĠchÆ¡iĊChá»§ĠnháºŃt,Ġ05/03/2017Ġ16:53ĠPMĠGMT+7Ċ(VTCĠNews)Ġ-ĠXÃ´ĠxÃ¡tĠvá»ĽiĠnhÃ³mĠbáº¡nĠÄĳangĠÄĳÃ¹aĠnghá»ĭchĠtrÃªnĠsÃ¢nĠtrÆ°á»Ŀng,ĠDĠquayĠlÆ°ngĠláº¡iĠthÃ¬Ġbáº¥tĠngá»ĿĠbá»ĭĠbáº¡nĠÄĳÃ¢mĠtrá»įngĠthÆ°Æ¡ng.ĊNamĠsinhĠHÃłĠNá»ĻiĠÄĳÃ¢mĠbáº¡nĠcÃ¹ngĠlá»ĽpĠtá»ŃĠvongĊNgÃłyĠ5/3,ĠcÃ´ngĠanĠTPĠQuyĠNhÆ¡nĠchoĠbiáº¿t,ĠÄĳÆ¡nĠvá»ĭĠÄĳangĠtiáº¿nĠhÃłnhĠxÃ¡cĠminhĠvá»¥ĠemĠT.K.DĠ(SNĠ2003,Ġhá»įcĠsinhĠlá»ĽpĠ8)Ġbá»ĭĠbáº¡nĠÄĳÃ¢mĠtrá»įngĠthÆ°Æ¡ngĠtáº¡iĠtrÆ°á»Ŀng.ĊTheoĠthÃ´ngĠtinĠgiaĠÄĳÃ¬nh,Ġkhoáº£ngĠ15hĠngÃłyĠ2/3,ĠtrongĠgiá»ĿĠraĠchÆ¡i,ĠemĠT.K.DĠxáº£yĠraĠxÃ´ĠxÃ¡tĠvá»ĽiĠbáº¡nĠT.N.T.P.ĠNgayĠkhiĠDĠquayĠÄĳiĠthÃ¬Ġbáº¥tĠngá»ĿĠbá»ĭĠbáº¡nĠP.ĠdÃ¹ngĠdaoĠÄĳÃ¢mĠvÃłoĠlÆ°ngĠtrá»įngĠthÆ°Æ¡ng.ĠNgayĠláºŃpĠtá»©c,ĠDĠÄĳÆ°á»£cĠtháº§yĠcÃ´ĠgiÃ¡oĠÄĳÆ°aĠÄĳáº¿nĠbá»ĩnhĠviá»ĩnĠÄĲaĠkhoaĠTPĠQuyĠNhÆ¡nĠcáº¥pĠcá»©u.ĊAnhĠTráº§nĠNgá»įcĠDÅ©ngĠ(bá»ĳĠemĠD)ĠchoĠbiáº¿t,Ġhiá»ĩnĠtáº¡iĠDĠÄĳÃ£ĠquaĠcÆ¡nĠnguyĠká»ĭchĠnhÆ°ngĠváº«nĠpháº£iĠnáº±mĠviá»ĩnĠÄĳá»ĥĠtiáº¿pĠtá»¥cĠÄĳiá»ģuĠtrá»ĭ.Ċ\"SauĠkhiĠvá»¥Ġviá»ĩcĠxáº£yĠra,ĠgiaĠÄĳÃ¬nhĠemĠPĠcÃ³ĠÄĳáº¿nĠbá»ĩnhĠviá»ĩnĠÄĳá»ĥĠthÄĥmĠhá»ıi.ĠVá»¥Ġviá»ĩcĠváº«nĠÄĳangĠÄĳÆ°á»£cĠcÆ¡ĠquanĠcÃ´ngĠanĠlÃłmĠrÃµ\",ĠanhĠDÅ©ngĠchoĠhay.<SEP>Ã²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²nÃ²n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([tokenizer.id_to_token(id) for id in tokens[1].ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode_batch([token.ids for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Molypden lần đầu được khai thác ở đây từ cuối những năm 60 đến cuối những năm 70, nhưng dần thoái trào khi lợi nhuận giảm xuống. Đến cuối thập kỷ, giá đã tăng trở lại lần nữa khi các khu mỏ giàu molypden ở Alaska, British Columbia và miền tây nước Mỹ ...\\nĐọc thêm\\nNgôi nhà gạch ngập tràn hạnh phúc của vợ chồng trẻ và 2 ...\\nNgôi nhà gạch ngập tràn hạnh phúc của vợ chồng trẻ và 2 cô con gái. Với mong muốn 2 cô con gái có môi trường vui chơi, phát triển toàn diện, đôi vợ chồng trẻ đã quyết định xây dựng ngôi nhà mới theo sở thích của 2 cô bé. Tên của 2 cô con gái Chi và Vi, gộp lại và ...\\nĐọc thêm\\nCÁC BÀI SUY NIỆM CHÚA NHẬT 16 THƯỜNG NIÊN. NĂM ...\\n2019-7-19\\u2002·\\u2002Lời Chúa hôm nay tiếp tục khai triển đề tài này, đặc biệt qua câu chuyện Chúa đến thăm chị em cô Mác-ta và Ma-ri-a tại nhà các cô. Để giúp chúng ta biết quý trọng sự viếng thăm của Chúa, Phụng vụ Lời Chúa lấy lại câu chuyện Đức Chúa ghé thăm ông Áp …\\nĐọc thêm\\nthiết bị nghiền simex\\nGiới thiệu chung Công ty NGUYỄN VINH là nhà cung cấp độc quyền của hãng hàng đầu thế giới với công nghệ tiên tiến nhất về thiết bị chế biến khoáng sản, thực phẩm, dược phẩm, hóa chất HOSOKAWA & thiết bị nghiền sàng vật liệu xây dựng, khoáng\\nĐọc thêm\\nCác Bài Suy Niệm Chúa Nhật XVI Năm C\\n2019-7-19\\u2002·\\u2002Ngày 30 tháng 1 năm 1956, nhà riêng của mục sư King bị đánh bom. Một đám đông những người da đen ủng hộ ông tỏ ra giận dữ và tụ tập trên con đường trước ngôi nhà, tự vũ trang với dao, súng, gậy gộc, đá và chai lọ để trả thù cho ông. Bấy giờ mục sư King nói với ...\\nĐọc thêm\\nSuy Niệm Tin Mừng Chúa Nhật 16 TN-C Bài 151-168 Maria ...\\n2019-7-21\\u2002·\\u2002Suy Niệm Tin Mừng Chúa Nhật 16 TN-C Bài 151-168 Maria chọn phần tốt - Detail - Tin Tức -...\\nĐọc thêm\\n#Dan La • Raw Ranked Sites\\nbahasa Cari terjemahan dalam kamus dan nikmati belajar bahasa dengan les kosakata dan banyak permainan gratis ... mua acc gunny 2017, play.home gunny , gunny ii, gunny lau 360 , cách lấy lại tài khoản gunny mobi, s1gunny gunny mien ...\\nĐọc thêm\\nNhà Phố Vạn Phúc\\nCác sản phẩm nội thất trong phong cách có kiểu dáng đơn giản, sử dụng những tông màu trắng, để tạo cảm giác thoải mái cho người nhìn. Phố Xinh sử dụng tông màu trắng làm chủ đạo nên căn phòng lúc nào cũng mang đến sự quyến rũ, cuốn hút nhất.\\nĐọc thêm\\nđá vụn đá vụn\\nĐá thạch anh vụn mua ở đâu có tác dụng gì – Mặt phật đá ... Thạch Anh Vụn có giá rất rẻ nếu bạn dùng đá thạch anh thô không qua sàng lọc.trên thị trường có vô vàn giá bán thạch anh vụn.Nhưng và đem lại hiệu quả cao cho người sử dụng, nên dùng các loại Đá Thạch Anh Vụn của cơ sở sản xuất chế tác ...\\nĐọc thêm\\nHệ vi sinh vật đường ruột: định nghĩa, tầm quan trọng và sử ...\\n2021-8-13\\u2002·\\u2002Cơ thể con người là nơi cư trú của hàng tỷ vi khuẩn hoặc vi trùng. Một số trong số chúng có ích và một số có hại. hiểu biết nhiều hơn về các vi sinh vật trong c\\nĐọc thêm\\nnghiền đá ở bangladesh\\nmáy nghiền bột ở bangladesh – máy nghiền sàng đá, bán máy nghiền, đá vôi là tài nguyên khoáng sản lớn ở bangladesh. máy nghiền đví dụ về nghiên cứu nghiền và nghiền khoáng sản than và ô nhiễmá vôi và đá vôi máy nghiền đóng một vai trò quan trọng ...\\nĐọc thêm\\nHome\\nKhoai tây rất giàu vitamin và khoáng chất như B6, vitamin C, kali,… có khả năng kích thích sản sinh collagen nuôi dưỡng làn da. Mặt nạ khoai tây giúp làm trắng da mịn màng, đàn hồi hơn và nhanh chóng làm mờ các vết nám, tàn nhang.',\n",
       " 'Nam sinh bị bạn đâm trọng thương\\nNam sinh bị bạn đâm trọng thương trong giờ ra chơi\\nChủ nhật, 05/03/2017 16:53 PM GMT+7\\n(VTC News) - Xô xát với nhóm bạn đang đùa nghịch trên sân trường, D quay lưng lại thì bất ngờ bị bạn đâm trọng thương.\\nNam sinh Hà Nội đâm bạn cùng lớp tử vong\\nNgày 5/3, công an TP Quy Nhơn cho biết, đơn vị đang tiến hành xác minh vụ em T.K.D (SN 2003, học sinh lớp 8) bị bạn đâm trọng thương tại trường.\\nTheo thông tin gia đình, khoảng 15h ngày 2/3, trong giờ ra chơi, em T.K.D xảy ra xô xát với bạn T.N.T.P. Ngay khi D quay đi thì bất ngờ bị bạn P. dùng dao đâm vào lưng trọng thương. Ngay lập tức, D được thầy cô giáo đưa đến bệnh viện Đa khoa TP Quy Nhơn cấp cứu.\\nAnh Trần Ngọc Dũng (bố em D) cho biết, hiện tại D đã qua cơn nguy kịch nhưng vẫn phải nằm viện để tiếp tục điều trị.\\n\"Sau khi vụ việc xảy ra, gia đình em P có đến bệnh viện để thăm hỏi. Vụ việc vẫn đang được cơ quan công an làm rõ\", anh Dũng cho hay.ònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònònòn']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/tokenizers/issues/282"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
