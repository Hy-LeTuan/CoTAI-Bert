{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm \n",
    "from datasets import load_dataset\n",
    "from datasketch import MinHashLSH, MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c224784c70e459cb8fd00fdcb7db297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"./data_cleaned\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_hash(text, num_perm=128): \n",
    "    text = text[\"text\"].lower()\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for word in text.split(): \n",
    "        minhash.update(word.encode(\"utf-8\"))\n",
    "\n",
    "    return {\"hash\": minhash,}\n",
    "\n",
    "def calculate_lsh(hash_batch, query, threshold=0.9, num_perm=128): \n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for i, batch in enumerate(hash_batch): \n",
    "        lsh.insert(i, batch[\"hash\"])\n",
    "\n",
    "    result = lsh.query(query[\"hash\"])\n",
    "    returns = {\"duplicate\": []}\n",
    "\n",
    "    for i in range(len(hash_batch)): \n",
    "        returns[\"duplicate\"].append(True if i in result else False)\n",
    "\n",
    "    return returns\n",
    "\n",
    "def duplicate_filter(batch): \n",
    "    return [item[\"duplicate\"] != True for item in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(calculate_min_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_texts_to_file(unique_items, base_file_path=\"filtered_dataset\", file_path='unique_items.txt'):\n",
    "    with open(os.path.join(base_file_path, file_path), 'w', encoding='utf-8') as f:\n",
    "        for item in unique_items:\n",
    "            f.write(item + '\\n')\n",
    "    print(f\"Saved {len(unique_items)} unique items to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "790538it [06:51, 1840.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1571698it [13:39, 1336.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00001.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2361893it [20:36, 1285.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00002.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3158756it [27:36, 1342.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00003.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3930108it [34:35, 1354.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00004.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4714781it [41:33, 1978.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00005.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5506259it [48:29, 1934.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00006.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6291213it [55:23, 1296.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00007.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7085763it [1:02:38, 1837.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00008.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7886815it [1:09:56, 1287.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00009.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8693339it [1:17:16, 1284.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00010.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9484591it [1:24:23, 1416.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00011.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10284557it [1:31:15, 2028.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing LSH...\n",
      "Saved 500000 unique items to cleaned_00012.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11026756it [1:37:48, 1878.87it/s]\n"
     ]
    }
   ],
   "source": [
    "lsh = MinHashLSH(threshold=0.9, num_perm=128)\n",
    "unique_items = []\n",
    "current_size = 0\n",
    "file_counter = 0\n",
    "MAX_SIZE = 500000\n",
    "\n",
    "for i, item in tqdm(enumerate(dataset[\"train\"])): \n",
    "    minhash = item[\"hash\"]\n",
    "\n",
    "    if len(lsh.query(minhash)) == 0: \n",
    "        lsh.insert(i, minhash) \n",
    "        unique_items.append(item[\"text\"])\n",
    "        current_size += 1\n",
    "\n",
    "        if current_size >= MAX_SIZE: \n",
    "            print(f\"Refreshing LSH...\")\n",
    "            save_texts_to_file(unique_items=unique_items, file_path=f\"cleaned_{str(file_counter).zfill(5)}.txt\")\n",
    "\n",
    "            unique_items = []\n",
    "            lsh = MinHashLSH(threshold=0.9, num_perm=128)\n",
    "\n",
    "            current_size = 0\n",
    "            file_counter += 1\n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_texts_to_file(unique_items=unique_items, file_path=f\"cleaned_{str(13).zfill(5)}.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14670it [00:08, 1749.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])): \n\u001b[1;32m      3\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmap(calculate_lsh, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_perm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m})\n\u001b[1;32m      4\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mfilter(duplicate_filter, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasets/iterable_dataset.py:1389\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m   1391\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   1394\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   1395\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasets/iterable_dataset.py:679\u001b[0m, in \u001b[0;36mMappedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m ArrowExamplesIterable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_arrow, {})\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasets/iterable_dataset.py:752\u001b[0m, in \u001b[0;36mMappedExamplesIterable._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     function_args\u001b[38;5;241m.\u001b[39mappend(current_idx)\n\u001b[1;32m    751\u001b[0m transformed_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)  \u001b[38;5;66;03m# this will be updated with the function output\u001b[39;00m\n\u001b[0;32m--> 752\u001b[0m transformed_example\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# then we remove the unwanted columns\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_columns:\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mcalculate_min_hash\u001b[0;34m(text, num_perm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_min_hash\u001b[39m(text, num_perm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m): \n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     minhash \u001b[38;5;241m=\u001b[39m \u001b[43mMinHash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_perm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit(): \n\u001b[1;32m      5\u001b[0m         minhash\u001b[38;5;241m.\u001b[39mupdate(word\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasketch/minhash.py:106\u001b[0m, in \u001b[0;36mMinHash.__init__\u001b[0;34m(self, num_perm, seed, hashfunc, hashobj, hashvalues, permutations)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermutations \u001b[38;5;241m=\u001b[39m permutations\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermutations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_permutations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermutations[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumbers of hash values and permutations mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasketch/minhash.py:119\u001b[0m, in \u001b[0;36mMinHash._init_permutations\u001b[0;34m(self, num_perm)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_permutations\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_perm: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Create parameters for a random bijective permutation function\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# that maps a 32-bit hash value to another 32-bit hash value.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# http://en.wikipedia.org/wiki/Universal_hashing\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     gen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m--> 119\u001b[0m         [\n\u001b[1;32m    120\u001b[0m             (\n\u001b[1;32m    121\u001b[0m                 gen\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, _mersenne_prime, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint64),\n\u001b[1;32m    122\u001b[0m                 gen\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, _mersenne_prime, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint64),\n\u001b[1;32m    123\u001b[0m             )\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_perm)\n\u001b[1;32m    125\u001b[0m         ],\n\u001b[1;32m    126\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint64,\n\u001b[1;32m    127\u001b[0m     )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/datasketch/minhash.py:121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_permutations\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_perm: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Create parameters for a random bijective permutation function\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# that maps a 32-bit hash value to another 32-bit hash value.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# http://en.wikipedia.org/wiki/Universal_hashing\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     gen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    119\u001b[0m         [\n\u001b[1;32m    120\u001b[0m             (\n\u001b[0;32m--> 121\u001b[0m                 \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_mersenne_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    122\u001b[0m                 gen\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, _mersenne_prime, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint64),\n\u001b[1;32m    123\u001b[0m             )\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_perm)\n\u001b[1;32m    125\u001b[0m         ],\n\u001b[1;32m    126\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint64,\n\u001b[1;32m    127\u001b[0m     )\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_dataset = dataset[\"train\"]\n",
    "for i, example in tqdm(enumerate(dataset[\"train\"])): \n",
    "    new_dataset = new_dataset.map(calculate_lsh, batched=True, fn_kwargs={\"query\": example, \"threshold\": 0.9, \"num_perm\": 128})\n",
    "    new_dataset = new_dataset.filter(duplicate_filter, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'MinHashLSH' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m hashed_example \u001b[38;5;241m=\u001b[39m calculate_min_hash(example)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate LSH to find duplicates and decide whether to insert\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m lsh_result \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_lsh_for_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhashed_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlsh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# If not a duplicate, keep the example\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicate_filter(lsh_result):\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mcalculate_lsh_for_stream\u001b[0;34m(example, lsh, threshold, num_perm)\u001b[0m\n\u001b[1;32m     13\u001b[0m is_duplicate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_duplicate:\n\u001b[0;32m---> 15\u001b[0m     lsh\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlsh\u001b[49m\u001b[43m)\u001b[49m, example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m: is_duplicate}\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'MinHashLSH' has no len()"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def calculate_min_hash(text, num_perm=128):\n",
    "    text = text[\"text\"]\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        minhash.update(word.encode(\"utf-8\"))\n",
    "    return {\"hash\": minhash}\n",
    "\n",
    "def calculate_lsh_for_stream(example, lsh, threshold=0.9, num_perm=128):\n",
    "    result = lsh.query(example[\"hash\"])\n",
    "    is_duplicate = len(result) > 0\n",
    "    if not is_duplicate:\n",
    "        lsh.insert(len(lsh), example[\"hash\"])\n",
    "    return {\"duplicate\": is_duplicate}\n",
    "\n",
    "def duplicate_filter(row):\n",
    "    return not row[\"duplicate\"]\n",
    "\n",
    "# Initialize LSH for streaming processing\n",
    "lsh = MinHashLSH(threshold=0.9, num_perm=128)\n",
    "\n",
    "# Process the dataset in a streaming manner\n",
    "new_dataset = []\n",
    "for example in tqdm(dataset[\"train\"]):\n",
    "    # Calculate the MinHash for the current example\n",
    "    hashed_example = calculate_min_hash(example)\n",
    "    \n",
    "    # Calculate LSH to find duplicates and decide whether to insert\n",
    "    lsh_result = calculate_lsh_for_stream(hashed_example, lsh)\n",
    "    \n",
    "    # If not a duplicate, keep the example\n",
    "    if duplicate_filter(lsh_result):\n",
    "        new_dataset.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = new_dataset['train'].remove_columns([col for col in dataset['train'].column_names if col != 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.save_to_disk(\"filtered_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
