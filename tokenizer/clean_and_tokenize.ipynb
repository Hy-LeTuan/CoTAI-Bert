{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset \n",
    "import pyarrow as pa \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and process\n",
    "\n",
    "1. Loop through each files in the dataset and load it into memory\n",
    "2. Multi-process with all threads, then having an open stream to write into\n",
    "3. Save the table down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"../data_all/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer/tokenizer-50k\")\n",
    "tokenizer.model_max_length = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8d8a8f1e828ac8b3df9e3e0c7a935ca11dfff38900f4d69b37ba17d3826924f1\n",
      "8e896b46abd66d965883b5713eeb86732ef809d2f00056ff42008fb4414f7580\n",
      "baf1ac4fd46bc83e54d689349a1b3ddd6cc7ef18b3ccaf08a827428078ad6f54\n",
      "ee840529a3b85637222004772ab2f02a346821d70653fa0433241583a5f18ab3\n",
      "99af78acd141352a8264e51fc18c117f46e62e5348245cbfdc1ec9ef5b9c5afb\n",
      "ed3163e5bd9d45f63fe9b5ab1c0725af9a015e0f3b3f6d8438cd7f5a2ab9fe6d\n",
      "30ffdc7c4b728d3bc4250ed0f9b5bfad718a61924981986a3724a0cd0f86df9c\n",
      "c65ab14a3afdcb11be5cc93a14e0f95469fc9f749f5b51dfa9ae70ddb43ae45d\n",
      "63f2f2117a55ac4e647ef7a02510c2c1b925f2dbeebbe9bd85f8a0cb8a9f6e4e\n",
      "2602de87101aa4b4f8a93e56df0155219c0def1c0586a90a330267285783c31e\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from hashlib import sha256\n",
    "\n",
    "for i in range(10): \n",
    "    hashed = sha256(str(time.time()).encode(\"utf-8\"))\n",
    "    print(hashed.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_join(batch): \n",
    "    content = []\n",
    "    hashed = sha256(str(time.time()).encode(\"utf-8\")).hexdigest()\n",
    "    stream =  pa.CompressedOutputStream(f\"./temp/{hashed}.arrow\", compression=\"gzip\")\n",
    "\n",
    "    outputs = tokenizer.batch_encode_plus(batch[\"text\"])[\"input_ids\"]\n",
    "\n",
    "    for row in outputs: \n",
    "        # remove [SEP] token \n",
    "        row = row[:-1]\n",
    "\n",
    "        if len(row) <= 128: \n",
    "            content.append(row)\n",
    "        else: \n",
    "            for start_idx in range(0, len(row), 90): \n",
    "                content.append(row[start_idx:start_idx+128]) \n",
    "\n",
    "    table = pa.Table.from_arrays(\n",
    "        [pa.array(content)], names=[\"input_ids\"])\n",
    "\n",
    "    writer = pa.RecordBatchStreamWriter(stream, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def convert_to_tokens(batch, tokenizer): \n",
    "    outputs = tokenizer.batch_encode_plus(batch[\"text\"])[\"input_ids\"]\n",
    "    return {\"input_ids\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99d4ff5e5804a119fed3d799452ef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=14):   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [06:02, 120.85s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, file in tqdm(enumerate(sorted(os.listdir(base_folder)))): \n",
    "    if i < 3: \n",
    "        continue\n",
    "\n",
    "    dataset = load_dataset(\"arrow\", data_files=[os.path.join(base_folder, file)], split=\"train\", streaming=False, num_proc=16)\n",
    "    # dataset = dataset.map(convert_to_tokens, batched=True, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=[\"text\"], num_proc=14)\n",
    "    dataset = dataset.map(process_and_join, batched=True, num_proc=14, batch_size=10000, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7af3a3cc324bdaab1e536e1ccef76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da845b197b0643cc92711f094c36fd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f75c64eb18548d487bd5d507beb062c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow\n",
    "dataset = load_dataset(\"./temp\", num_proc=16, streaming=False, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 15545460\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5ba726c62b48aa822d1c60b6a11f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1555 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15017696336"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_parquet(\"./final/final_0000.parquet\", batch_size=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
