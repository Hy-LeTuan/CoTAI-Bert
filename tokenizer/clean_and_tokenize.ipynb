{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset \n",
    "import pyarrow as pa \n",
    "import time \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import pyarrow\n",
    "from hashlib import sha256\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and process\n",
    "\n",
    "1. Loop through each files in the dataset and load it into memory\n",
    "2. Multi-process with all threads, then having an open stream to write into\n",
    "3. Save the table down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"../data_all/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_tokenizer/tokenizer-50k\")\n",
    "tokenizer.model_max_length = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_join(batch, num_tokens_file): \n",
    "    content = []\n",
    "    token_length = 0\n",
    "\n",
    "    hashed = sha256(str(time.time()).encode(\"utf-8\")).hexdigest()\n",
    "    stream =  pa.CompressedOutputStream(f\"./temp/{hashed}.arrow\", compression=\"gzip\")\n",
    "    outputs = tokenizer.batch_encode_plus(batch[\"text\"])[\"input_ids\"]\n",
    "\n",
    "    for row in outputs: \n",
    "        # remove [SEP] token \n",
    "        row = row[:-1]\n",
    "\n",
    "        if len(row) <= 128: \n",
    "            content.append(row)\n",
    "            token_length += len(row)\n",
    "        else: \n",
    "            for start_idx in range(0, len(row), 90): \n",
    "                token_length += 128 \n",
    "                content.append(row[start_idx:start_idx+128]) \n",
    "\n",
    "    table = pa.Table.from_arrays(\n",
    "        [pa.array(content)], names=[\"input_ids\"])\n",
    "\n",
    "    writer = pa.RecordBatchStreamWriter(stream, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    num_tokens_file.write(f\"{token_length}\\n\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "def convert_to_tokens(batch, tokenizer): \n",
    "    outputs = tokenizer.batch_encode_plus(batch[\"text\"])[\"input_ids\"]\n",
    "    return {\"input_ids\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create files for status checking \n",
    "num_sentence_file = open(\"./num_sentence.txt\", \"a\", encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(base_folder)))): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19\u001b[39m: \n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for i, file in tqdm(enumerate(sorted(os.listdir(base_folder)))): \n",
    "    if i <= 19: \n",
    "        continue \n",
    "    num_tokens_file = open(f\"./num_tokens/token_{str(i).zfill(5)}.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    dataset = load_dataset(\"../data_all/data\", data_files=[file], split=\"train\", streaming=False, cache_dir=None)\n",
    "    dataset = dataset.map(process_and_join, batched=True, num_proc=14, fn_kwargs={\"num_tokens_file\": num_tokens_file}, batch_size=5000, remove_columns=[\"text\"])\n",
    "\n",
    "    num_sentence_file.write(f\"{len(dataset)}\\n\")\n",
    "    num_tokens_file.close()\n",
    "\n",
    "    dataset = load_dataset(\"./temp\", num_proc=16, streaming=False, split=\"train\", cache_dir=None)\n",
    "    dataset.to_parquet(f\"./final/final_{str(i).zfill(5)}.parquet\", batch_size=20000)\n",
    "\n",
    "    for file in os.listdir(\"./temp\"): \n",
    "        file_path = os.path.join(\"./temp\", file)\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)  \n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)  \n",
    "\n",
    "num_sentence_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
