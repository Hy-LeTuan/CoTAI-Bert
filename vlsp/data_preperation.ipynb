{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/17 07:58:30 WARN Utils: Your hostname, hyle-nitro resolves to a loopback address: 127.0.1.1; using 192.168.0.216 instead (on interface wlp0s20f3)\n",
      "24/11/17 07:58:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/17 07:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all .txt file paths in NER 2016 dataset\n",
    "# text_file_paths = sorted(glob(\"./data/NER2016-TestData-16-9-2016/**/*.txt\", recursive=True))\n",
    "text_file_paths = sorted(glob(\"./data/NER2016-TrainingData-3-3-2017-txt/**/*.txt\", recursive=True))\n",
    "origin = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = [\"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and extract tags\n",
    "def extract_and_reformat(text_file_paths):\n",
    "    for file_counter, filepath in enumerate(text_file_paths):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.readline()\n",
    "            output_file = open(f\"./data/formatted_data/2016/{origin}/{str(file_counter).zfill(5)}.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "            line_counter = 0\n",
    "\n",
    "            while content:\n",
    "                if line_counter <= 3:\n",
    "                    line_counter += 1\n",
    "                    content = f.readline()\n",
    "                    continue\n",
    "                # get tags\n",
    "                tags = content.split(\"\\t\")\n",
    "\n",
    "                # if tag is not a word\n",
    "                if tags[0] == '<s>' or tags[0] == \"</s>\":\n",
    "                    tags = []\n",
    "                else:\n",
    "                    tags[-1] = tags[-1].strip()\n",
    "\n",
    "                if tags and len(tags) >= 4:\n",
    "                    words = tags[0]\n",
    "                    ner_tag = tags[3]\n",
    "                    output_file.write(f\"{words} {ner_tag}\\n\")\n",
    "\n",
    "                content = f.readline()\n",
    "            \n",
    "            # prepare for next increment\n",
    "            output_file.close()\n",
    "\n",
    "extract_and_reformat(text_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(text_file_paths, lineSep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(lambda x: Row(length=len(x[\"value\"].split(\"\\t\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "     StructField(\"length\", IntegerType(), True)\n",
    "])\n",
    "df2 = rdd.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|length|\n",
      "+------+\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "|     5|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df2 = df2.collect()\n",
    "\n",
    "# select function returns another dataframe that can be used to show\n",
    "# df_word = df2.select(\"word\")\n",
    "\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
